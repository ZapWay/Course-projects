{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN6fQzeGRQpx",
        "outputId": "e2288068-6aab-4a5a-eb44-67df11600a26"
      },
      "outputs": [],
      "source": [
        "# Тут нужно скачать датасет с cityscapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gzqy4Ro7ReNy",
        "outputId": "261080fc-1592-41c7-eb4c-8a6345f4549d"
      },
      "outputs": [],
      "source": [
        "!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jko4UxZERj3T",
        "outputId": "813340db-ee45-45c4-9e94-87ced672daa7"
      },
      "outputs": [],
      "source": [
        "!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCXB3CIaip43",
        "outputId": "c9adfb56-f489-4c7d-d226-c47a50e6ac2b"
      },
      "outputs": [],
      "source": [
        "!unzip leftImg8bit_trainvaltest.zip -d ./cityscapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JQjNkZNiwkx",
        "outputId": "5873ff87-27a6-45bf-b8c3-764629e7e818"
      },
      "outputs": [],
      "source": [
        "!unzip -o gtFine_trainvaltest.zip -d ./cityscapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P9Eb4iPcjf4o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.v2 as T\n",
        "\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
        "\n",
        "from torchvision.tv_tensors import Image as TVImage, Mask, BoundingBoxes\n",
        "\n",
        "CS_LABELS_INFO_FULL = [\n",
        "    ('unlabeled',            0, 255, False),\n",
        "    ('ego vehicle',          1, 255, False),\n",
        "    ('rectification border', 2, 255, False),\n",
        "    ('out of roi',           3, 255, False),\n",
        "    ('static',               4, 255, False),\n",
        "    ('dynamic',              5, 255, False),\n",
        "    ('ground',               6, 255, False),\n",
        "    ('road',                 7, 0,   False),\n",
        "    ('sidewalk',             8, 1,   False),\n",
        "    ('parking',              9, 255, False),\n",
        "    ('rail track',          10, 255, False),\n",
        "    ('building',            11, 2,   False),\n",
        "    ('wall',                12, 3,   False),\n",
        "    ('fence',               13, 4,   False),\n",
        "    ('guard rail',          14, 255, False),\n",
        "    ('bridge',              15, 255, False),\n",
        "    ('tunnel',              16, 255, False),\n",
        "    ('pole',                17, 5,   False),\n",
        "    ('polegroup',           18, 5,   False),\n",
        "    ('traffic light',       19, 6,   False),\n",
        "    ('traffic sign',        20, 7,   False),\n",
        "    ('vegetation',          21, 8,   False),\n",
        "    ('terrain',             22, 9,   False),\n",
        "    ('sky',                 23, 10,  False),\n",
        "    ('person',              24, 11,  True),\n",
        "    ('rider',               25, 12,  True),\n",
        "    ('car',                 26, 13,  True),\n",
        "    ('truck',               27, 14,  True),\n",
        "    ('bus',                 28, 15,  True),\n",
        "    ('caravan',             29, 255, True),\n",
        "    ('trailer',             30, 255, True),\n",
        "    ('train',               31, 16,  True),\n",
        "    ('motorcycle',          32, 17,  True),\n",
        "    ('bicycle',             33, 18,  True),\n",
        "    ('license plate',       -1, 255, True)\n",
        "]\n",
        "\n",
        "LABELID_TO_TRAINID = {label[1]: label[2] for label in CS_LABELS_INFO_FULL}\n",
        "CITYSCAPES_CLASSES = [\n",
        "    (0, 'road', False), (1, 'sidewalk', False), (2, 'building', False),\n",
        "    (3, 'wall', False), (4, 'fence', False), (5, 'pole', False),\n",
        "    (6, 'traffic light', False), (7, 'traffic sign', False), (8, 'vegetation', False),\n",
        "    (9, 'terrain', False), (10, 'sky', False), (11, 'person', True),\n",
        "    (12, 'rider', True), (13, 'car', True), (14, 'truck', True),\n",
        "    (15, 'bus', True), (16, 'train', True), (17, 'motorcycle', True),\n",
        "    (18, 'bicycle', True), (255, 'void', False)\n",
        "]\n",
        "NUM_CITYSCAPES_CLASSES = 19 \n",
        "CITYSCAPES_THING_CLASSES_TRAIN_IDS = [info[0] for info in CITYSCAPES_CLASSES if info[2] and info[0] != 255]\n",
        "CITYSCAPES_THING_MAP = {train_id: i for i, train_id in enumerate(CITYSCAPES_THING_CLASSES_TRAIN_IDS)}\n",
        "CITYSCAPES_THING_INV_MAP = {i: train_id for train_id, i in CITYSCAPES_THING_MAP.items()}\n",
        "NUM_CITYSCAPES_THING_CLASSES = len(CITYSCAPES_THING_CLASSES_TRAIN_IDS)\n",
        "\n",
        "CITYSCAPES_IGNORE_INDEX = 255\n",
        "CITYSCAPES_PANOPTIC_OFFSET = 1000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Функция для вычисления IoU по маске"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OKfKDSnKyqt_"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(mask1, mask2):\n",
        "    mask1 = mask1.bool()\n",
        "    mask2 = mask2.bool()\n",
        "    intersection = (mask1 & mask2).sum().float()\n",
        "    union = (mask1 | mask2).sum().float()\n",
        "    if union == 0:\n",
        "        return 0.0\n",
        "    return (intersection / union).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ony5HN_XyxG3"
      },
      "outputs": [],
      "source": [
        "def calculate_pq(gt_panoptic_map_np, pred_panoptic_map_tensor,\n",
        "                                         iou_threshold=0.5):\n",
        "    pred_panoptic_map_np = pred_panoptic_map_tensor.cpu().numpy()\n",
        "    gt_seg_ids = np.unique(gt_panoptic_map_np)\n",
        "    pred_seg_ids = np.unique(pred_panoptic_map_np)\n",
        "    parsed_gt_segments = []\n",
        "    for gt_comb_id in gt_seg_ids:\n",
        "        if gt_comb_id == 0 and CITYSCAPES_PANOPTIC_OFFSET == 0:\n",
        "            continue \n",
        "        gt_mask = (gt_panoptic_map_np == gt_comb_id)\n",
        "        if gt_mask.sum() == 0: continue\n",
        "        semantic_id = gt_comb_id // CITYSCAPES_PANOPTIC_OFFSET\n",
        "        instance_id = gt_comb_id % CITYSCAPES_PANOPTIC_OFFSET\n",
        "        is_thing = any(c[0] == semantic_id and c[2] for c in CITYSCAPES_CLASSES)\n",
        "        if semantic_id == CITYSCAPES_IGNORE_INDEX:\n",
        "            continue\n",
        "        parsed_gt_segments.append({\n",
        "            'combined_id': gt_comb_id,\n",
        "            'mask': torch.from_numpy(gt_mask),\n",
        "            'category_id': semantic_id,\n",
        "            'instance_id': instance_id,\n",
        "            'isthing': is_thing\n",
        "        })\n",
        "    parsed_pred_segments = []\n",
        "    for pred_comb_id in pred_seg_ids:\n",
        "        if pred_comb_id == 0 and CITYSCAPES_PANOPTIC_OFFSET == 0:\n",
        "            continue\n",
        "        pred_mask = (pred_panoptic_map_np == pred_comb_id)\n",
        "        if pred_mask.sum() == 0: continue\n",
        "        semantic_id = pred_comb_id // CITYSCAPES_PANOPTIC_OFFSET\n",
        "        instance_id = pred_comb_id % CITYSCAPES_PANOPTIC_OFFSET\n",
        "        is_thing = any(c[0] == semantic_id and c[2] for c in CITYSCAPES_CLASSES)\n",
        "        if semantic_id == CITYSCAPES_IGNORE_INDEX:\n",
        "            continue\n",
        "        parsed_pred_segments.append({\n",
        "            'combined_id': pred_comb_id,\n",
        "            'mask': torch.from_numpy(pred_mask),\n",
        "            'category_id': semantic_id,\n",
        "            'instance_id': instance_id,\n",
        "            'isthing': is_thing\n",
        "        })\n",
        "    pq_per_class = {}\n",
        "    all_train_ids = sorted(list(set(s['category_id'] for s in parsed_gt_segments + parsed_pred_segments)))\n",
        "    for category_id in all_train_ids:\n",
        "        if category_id == CITYSCAPES_IGNORE_INDEX:\n",
        "            continue\n",
        "        gt_class_segments = [s for s in parsed_gt_segments if s['category_id'] == category_id]\n",
        "        pred_class_segments = [s for s in parsed_pred_segments if s['category_id'] == category_id]\n",
        "        is_current_class_thing = any(c[0] == category_id and c[2] for c in CITYSCAPES_CLASSES)\n",
        "        if not gt_class_segments and not pred_class_segments:\n",
        "            continue \n",
        "        tp, fp, fn = 0, 0, 0\n",
        "        sum_iou_of_tp = 0.0\n",
        "        if is_current_class_thing:\n",
        "            matched_gt_indices = set()\n",
        "            for pred_idx, pred_seg in enumerate(pred_class_segments):\n",
        "                best_iou = 0.0\n",
        "                best_gt_idx = -1\n",
        "                for gt_idx, gt_seg in enumerate(gt_class_segments):\n",
        "                    if gt_idx in matched_gt_indices:\n",
        "                        continue\n",
        "                    iou = calculate_iou(pred_seg['mask'], gt_seg['mask'])\n",
        "                    if iou > best_iou:\n",
        "                        best_iou = iou\n",
        "                        best_gt_idx = gt_idx\n",
        "\n",
        "                if best_iou > iou_threshold and best_gt_idx != -1:\n",
        "                    tp += 1\n",
        "                    sum_iou_of_tp += best_iou\n",
        "                    matched_gt_indices.add(best_gt_idx)\n",
        "                else:\n",
        "                    fp += 1\n",
        "\n",
        "            fn = len(gt_class_segments) - len(matched_gt_indices)\n",
        "        else:\n",
        "            gt_stuff_mask_class = torch.zeros_like(parsed_gt_segments[0]['mask'] if parsed_gt_segments else parsed_pred_segments[0]['mask'], dtype=torch.bool)\n",
        "            for gt_seg in gt_class_segments: gt_stuff_mask_class |= gt_seg['mask']\n",
        "            pred_stuff_mask_class = torch.zeros_like(gt_stuff_mask_class, dtype=torch.bool)\n",
        "            for pred_seg in pred_class_segments: pred_stuff_mask_class |= pred_seg['mask']\n",
        "            if gt_stuff_mask_class.sum() > 0 or pred_stuff_mask_class.sum() > 0:\n",
        "                iou_stuff = calculate_iou(pred_stuff_mask_class, gt_stuff_mask_class)\n",
        "                if iou_stuff > 0:\n",
        "                    tp = 1\n",
        "                    sum_iou_of_tp = iou_stuff\n",
        "                    if gt_stuff_mask_class.sum() == 0 and pred_stuff_mask_class.sum() > 0: fp = 1; tp=0; sum_iou_of_tp=0\n",
        "                    if pred_stuff_mask_class.sum() == 0 and gt_stuff_mask_class.sum() > 0: fn = 1; tp=0; sum_iou_of_tp=0\n",
        "\n",
        "                elif pred_stuff_mask_class.sum() > 0:\n",
        "                    fp = 1\n",
        "                elif gt_stuff_mask_class.sum() > 0:\n",
        "                    fn = 1\n",
        "\n",
        "        sq = sum_iou_of_tp / tp if tp > 0 else 0.0\n",
        "        rq_denominator = (2 * tp + fp + fn)\n",
        "        rq = (2 * tp) / rq_denominator if rq_denominator > 0 else 0.0\n",
        "\n",
        "        pq = sq * rq\n",
        "        pq_per_class[category_id] = {'pq': pq, 'sq': sq, 'rq': rq, 'tp': tp, 'fp': fp, 'fn': fn}\n",
        "    final_pq = 0.0\n",
        "    final_sq = 0.0\n",
        "    final_rq = 0.0\n",
        "    num_classes_for_avg = 0\n",
        "\n",
        "    for cat_id, metrics in pq_per_class.items():\n",
        "        is_gt_present_for_class = any(s['category_id'] == cat_id for s in parsed_gt_segments)\n",
        "        if is_gt_present_for_class:\n",
        "            final_pq += metrics['pq']\n",
        "            final_sq += metrics['sq']\n",
        "            final_rq += metrics['rq']\n",
        "            num_classes_for_avg += 1\n",
        "\n",
        "    if num_classes_for_avg > 0:\n",
        "        final_pq /= num_classes_for_avg\n",
        "        final_sq /= num_classes_for_avg\n",
        "        final_rq /= num_classes_for_avg\n",
        "\n",
        "    return final_pq, final_sq, final_rq, pq_per_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2jI8XB8NjzhU"
      },
      "outputs": [],
      "source": [
        "class CityscapesPanopticDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transforms=None, max_samples=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transforms = transforms\n",
        "        self.img_dir = os.path.join(root_dir, 'leftImg8bit', split)\n",
        "        self.ann_dir = os.path.join(root_dir, 'gtFine', split)\n",
        "        self.images = []\n",
        "        self.labelid_map_files = []\n",
        "        self.instanceid_map_files = []\n",
        "\n",
        "        for city in os.listdir(self.img_dir):\n",
        "            city_img_dir = os.path.join(self.img_dir, city)\n",
        "            city_ann_dir = os.path.join(self.ann_dir, city)\n",
        "            for file_name in os.listdir(city_img_dir):\n",
        "                if file_name.endswith('_leftImg8bit.png'):\n",
        "                    img_path = os.path.join(city_img_dir, file_name)\n",
        "                    base_ann_name = file_name.replace('_leftImg8bit.png', '_gtFine')\n",
        "                    labelid_png_name = f\"{base_ann_name}_labelIds.png\"\n",
        "                    instanceid_png_name = f\"{base_ann_name}_instanceIds.png\"\n",
        "                    labelid_path = os.path.join(city_ann_dir, labelid_png_name)\n",
        "                    instanceid_path = os.path.join(city_ann_dir, instanceid_png_name)\n",
        "                    if os.path.exists(labelid_path) and os.path.exists(instanceid_path):\n",
        "                        self.images.append(img_path)\n",
        "                        self.labelid_map_files.append(labelid_path)\n",
        "                        self.instanceid_map_files.append(instanceid_path)\n",
        "        if max_samples:\n",
        "            indices = list(range(len(self.images)))\n",
        "            random.shuffle(indices)\n",
        "            selected_indices = indices[:max_samples]\n",
        "            self.images = [self.images[i] for i in selected_indices]\n",
        "            self.labelid_map_files = [self.labelid_map_files[i] for i in selected_indices]\n",
        "            self.instanceid_map_files = [self.instanceid_map_files[i] for i in selected_indices]\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    def _generate_panoptic_targets(self, labelid_map_path, instanceid_map_path):\n",
        "        labelid_img_pil = Image.open(labelid_map_path)\n",
        "        instance_img_pil = Image.open(instanceid_map_path)\n",
        "        labelid_map_np = np.array(labelid_img_pil)\n",
        "        instanceid_map_np = np.array(instance_img_pil)\n",
        "\n",
        "        semantic_map_trainid_np = np.full(labelid_map_np.shape, CITYSCAPES_IGNORE_INDEX, dtype=np.uint8)\n",
        "        for cityscapes_id, train_id_val in LABELID_TO_TRAINID.items():\n",
        "            semantic_map_trainid_np[labelid_map_np == cityscapes_id] = train_id_val\n",
        "        boxes_np, labels_np, masks_np = [], [], []\n",
        "        gt_panoptic_eval_map = np.zeros_like(semantic_map_trainid_np, dtype=np.int32)\n",
        "        gt_segments_info_for_pq = []\n",
        "        current_gt_instance_id_counter = 1 \n",
        "        unique_instance_values = np.unique(instanceid_map_np)\n",
        "        for inst_val in unique_instance_values:\n",
        "            original_label_id_gt = inst_val // 1000 \n",
        "            \n",
        "            train_id_gt = LABELID_TO_TRAINID.get(original_label_id_gt, CITYSCAPES_IGNORE_INDEX)\n",
        "            if train_id_gt == CITYSCAPES_IGNORE_INDEX:\n",
        "                continue\n",
        "            is_thing_gt = any(c[0] == train_id_gt and c[2] for c in CITYSCAPES_CLASSES)\n",
        "            mask_region_gt = (instanceid_map_np == inst_val)\n",
        "            combined_id_gt = 0\n",
        "            if is_thing_gt:\n",
        "                combined_id_gt = train_id_gt * CITYSCAPES_PANOPTIC_OFFSET + current_gt_instance_id_counter\n",
        "                gt_panoptic_eval_map[mask_region_gt] = combined_id_gt\n",
        "                current_gt_instance_id_counter +=1\n",
        "                if train_id_gt in CITYSCAPES_THING_CLASSES_TRAIN_IDS:\n",
        "                    binary_mask_for_instance = mask_region_gt.astype(np.uint8)\n",
        "                    pos = np.where(binary_mask_for_instance)\n",
        "                    if len(pos[0]) > 0 and len(pos[1]) > 0:\n",
        "                        xmin, xmax = np.min(pos[1]), np.max(pos[1])\n",
        "                        ymin, ymax = np.min(pos[0]), np.max(pos[0])\n",
        "                        if xmax > xmin and ymax > ymin:\n",
        "                            boxes_np.append([xmin, ymin, xmax, ymax])\n",
        "                            labels_np.append(CITYSCAPES_THING_MAP[train_id_gt])\n",
        "                            masks_np.append(binary_mask_for_instance)\n",
        "            else:\n",
        "                combined_id_gt = train_id_gt * CITYSCAPES_PANOPTIC_OFFSET\n",
        "                gt_panoptic_eval_map[mask_region_gt] = combined_id_gt\n",
        "            if combined_id_gt != 0:\n",
        "                 gt_segments_info_for_pq.append({\n",
        "                     'id': combined_id_gt,\n",
        "                     'category_id': train_id_gt,\n",
        "                     'isthing': is_thing_gt\n",
        "                 })\n",
        "\n",
        "        return semantic_map_trainid_np, boxes_np, labels_np, masks_np, gt_panoptic_eval_map, gt_segments_info_for_pq\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        labelid_map_path = self.labelid_map_files[idx]\n",
        "        instanceid_map_path = self.instanceid_map_files[idx]\n",
        "        image_pil = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        semantic_map_np, boxes_np, labels_np, masks_np, gt_panoptic_eval_map, gt_segments_info_for_pq = \\\n",
        "            self._generate_panoptic_targets(labelid_map_path, instanceid_map_path)\n",
        "\n",
        "        image_tv = TVImage(image_pil)\n",
        "        semantic_target_tv = Mask(semantic_map_np.astype(np.int64))\n",
        "\n",
        "        h, w = image_pil.size[1], image_pil.size[0]\n",
        "        if len(boxes_np) > 0:\n",
        "            boxes_tv = BoundingBoxes(np.array(boxes_np, dtype=np.float32), format=\"XYXY\", canvas_size=(h,w))\n",
        "            labels_tv = torch.tensor(labels_np, dtype=torch.int64)\n",
        "            masks_tv = Mask(np.array(masks_np, dtype=np.uint8))\n",
        "        else:\n",
        "            boxes_tv = BoundingBoxes(torch.empty((0, 4), dtype=torch.float32), format=\"XYXY\", canvas_size=(h,w))\n",
        "            labels_tv = torch.empty((0,), dtype=torch.int64)\n",
        "            masks_tv = Mask(torch.empty((0, h, w), dtype=np.uint8))\n",
        "\n",
        "        instance_target = {\"boxes\": boxes_tv, \"labels\": labels_tv, \"masks\": masks_tv}\n",
        "        if self.transforms:\n",
        "            image_tv_transformed, semantic_target_tv_transformed, instance_target_transformed = \\\n",
        "                self.transforms(image_tv, semantic_target_tv, instance_target)\n",
        "            return image_tv_transformed, semantic_target_tv_transformed, instance_target_transformed, \\\n",
        "                   image_tv, gt_panoptic_eval_map, gt_segments_info_for_pq\n",
        "        else:\n",
        "            return image_tv, semantic_target_tv, instance_target, \\\n",
        "                   image_tv, gt_panoptic_eval_map, gt_segments_info_for_pq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qMXFN9sM8ytK"
      },
      "outputs": [],
      "source": [
        "def get_cityscapes_color_map():\n",
        "    colors = np.array([\n",
        "        [128, 64, 128], [244, 35, 232], [70, 70, 70], [102, 102, 156], [190, 153, 153],\n",
        "        [153, 153, 153], [250, 170, 30], [220, 220, 0], [107, 142, 35], [152, 251, 152],\n",
        "        [70, 130, 180], [220, 20, 60], [255, 0, 0], [0, 0, 142], [0, 0, 70],\n",
        "        [0, 60, 100], [0, 80, 100], [0, 0, 230], [119, 11, 32], [0,0,0]\n",
        "    ], dtype=np.uint8)\n",
        "    full_colors = np.zeros((256, 3), dtype=np.uint8)\n",
        "    full_colors[:len(colors)] = colors\n",
        "    return full_colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rvcntWmp5eqs",
        "outputId": "12f42b49-25d8-43d9-9378-9ebdd149fd32"
      },
      "outputs": [],
      "source": [
        "CITYSCAPES_ROOT = \"./cityscapes\"\n",
        "MAX_SAMPLES_FOR_DATASET_DEMO = 5 \n",
        "IMG_SIZE_FOR_DATASET_DEMO = (256, 512)\n",
        "demo_transforms = T.Compose([\n",
        "    T.Resize(IMG_SIZE_FOR_DATASET_DEMO, antialias=True),\n",
        "    T.ToDtype(torch.float, scale=True),\n",
        "    # T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset_for_viz = CityscapesPanopticDataset(\n",
        "    CITYSCAPES_ROOT,\n",
        "    'train',\n",
        "    transforms=demo_transforms,\n",
        "    max_samples=MAX_SAMPLES_FOR_DATASET_DEMO\n",
        ")\n",
        "\n",
        "num_samples_to_viz = min(3, len(dataset_for_viz))\n",
        "random_indices = random.sample(range(len(dataset_for_viz)), num_samples_to_viz)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dABaVbjcj1bt"
      },
      "outputs": [],
      "source": [
        "def get_transform(train, new_size=(256, 512)):\n",
        "    transforms = []\n",
        "    transforms.append(T.Resize(new_size, antialias=True))\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
        "    transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images_transformed = torch.stack([item[0] for item in batch], 0)\n",
        "    semantic_targets_transformed = torch.stack([item[1] for item in batch], 0)\n",
        "    instance_targets_transformed = [item[2] for item in batch]\n",
        "    original_images_for_eval = [item[3] for item in batch]\n",
        "    gt_panoptic_maps_for_eval = [item[4] for item in batch]\n",
        "    gt_segments_info_list_for_eval = [item[5] for item in batch]\n",
        "    return images_transformed, semantic_targets_transformed, instance_targets_transformed, \\\n",
        "           original_images_for_eval, gt_panoptic_maps_for_eval, gt_segments_info_list_for_eval\n",
        "\n",
        "def get_semantic_model(num_classes=NUM_CITYSCAPES_CLASSES):\n",
        "    model = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
        "    model.classifier[-1] = nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
        "    if hasattr(model, 'aux_classifier') and model.aux_classifier is not None:\n",
        "         model.aux_classifier[-1] = nn.Conv2d(256, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "    return model\n",
        "\n",
        "def get_instance_model(num_thing_classes=NUM_CITYSCAPES_THING_CLASSES):\n",
        "    model = maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
        "    in_features_box = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features_box, num_thing_classes + 1)\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer, num_thing_classes + 1)\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_semantic_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=CITYSCAPES_IGNORE_INDEX)\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch} [Semantic Training]\", unit=\"batch\")\n",
        "\n",
        "    for i, (images, semantic_targets, _) in enumerate(progress_bar):\n",
        "        images = images.to(device)\n",
        "        semantic_targets = semantic_targets.squeeze(1).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)['out']\n",
        "        loss = criterion(outputs, semantic_targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss = loss.item()\n",
        "        total_loss += current_loss\n",
        "        progress_bar.set_postfix(loss=f\"{current_loss:.4f}\", avg_loss=f\"{total_loss/(i+1):.4f}\")\n",
        "\n",
        "    avg_epoch_loss = total_loss / len(data_loader)\n",
        "    print(f\"Epoch {epoch} Semantic Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "    return avg_epoch_loss\n",
        "\n",
        "def train_instance_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    iter_count = 0\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch} [Instance Training]\", unit=\"batch\")\n",
        "    for i, (images, _, instance_targets) in enumerate(progress_bar):\n",
        "        images_on_device = [img.to(device) for img in images]\n",
        "        targets_on_device = []\n",
        "        for t_dict in instance_targets:\n",
        "            targets_on_device.append({k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t_dict.items()})\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images_on_device, targets_on_device)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        current_total_loss = losses.item()\n",
        "        total_loss += current_total_loss\n",
        "        iter_count += 1\n",
        "        loss_details_str = \", \".join([f\"{k}: {v.item():.3f}\" for k, v in loss_dict.items()])\n",
        "        progress_bar.set_postfix(total_loss=f\"{current_total_loss:.4f}\", avg_loss=f\"{total_loss/iter_count:.4f}\", details=loss_details_str)\n",
        "\n",
        "    if iter_count > 0:\n",
        "        avg_epoch_loss = total_loss / iter_count\n",
        "        print(f\"Epoch {epoch} Instance Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "        return avg_epoch_loss\n",
        "    else:\n",
        "        print(f\"Epoch {epoch} Instance: No valid batches processed.\")\n",
        "        return float('nan')\n",
        "\n",
        "\n",
        "def panoptic_fusion(semantic_pred, instance_preds):\n",
        "    panoptic_seg = torch.zeros_like(semantic_pred, dtype=torch.int32)\n",
        "    H, W = semantic_pred.shape\n",
        "    for train_id in range(NUM_CITYSCAPES_CLASSES):\n",
        "        is_thing = any(c[0] == train_id and c[2] for c in CITYSCAPES_CLASSES)\n",
        "        if not is_thing:\n",
        "            panoptic_seg[semantic_pred == train_id] = train_id * CITYSCAPES_PANOPTIC_OFFSET\n",
        "    sorted_instances = sorted(\n",
        "        [\n",
        "            {\n",
        "                'mask': inst['masks'].squeeze(0),\n",
        "                'label': CITYSCAPES_THING_INV_MAP[inst['labels'].item()],\n",
        "                'score': inst['scores'].item()\n",
        "            } for inst in instance_preds\n",
        "        ],\n",
        "        key=lambda x: x['score'],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    used_instance_pixels = torch.zeros_like(semantic_pred, dtype=torch.bool)\n",
        "    current_instance_id = 1\n",
        "\n",
        "    for inst in sorted_instances:\n",
        "        mask = inst['mask'] > 0.5\n",
        "        label_train_id = inst['label']\n",
        "        current_mask_region = mask & ~used_instance_pixels\n",
        "        if current_mask_region.sum() == 0:\n",
        "            continue\n",
        "        panoptic_seg[current_mask_region] = label_train_id * CITYSCAPES_PANOPTIC_OFFSET + current_instance_id\n",
        "        used_instance_pixels |= current_mask_region\n",
        "        current_instance_id += 1\n",
        "        if current_instance_id >= CITYSCAPES_PANOPTIC_OFFSET:\n",
        "            break\n",
        "    remaining_pixels = ~used_instance_pixels\n",
        "    for train_id in CITYSCAPES_THING_CLASSES_TRAIN_IDS:\n",
        "        semantic_thing_pixels = (semantic_pred == train_id) & remaining_pixels\n",
        "        panoptic_seg[semantic_thing_pixels] = train_id * CITYSCAPES_PANOPTIC_OFFSET\n",
        "    return panoptic_seg\n",
        "\n",
        "\n",
        "\n",
        "def predict_panoptic(semantic_model, instance_model, image_tensor, device, instance_score_thresh=0.5):\n",
        "    semantic_model.eval()\n",
        "    instance_model.eval()\n",
        "    with torch.no_grad():\n",
        "        sem_output = semantic_model(image_tensor.unsqueeze(0).to(device))['out']\n",
        "        semantic_pred = torch.argmax(sem_output.squeeze(), dim=0).cpu()\n",
        "        inst_outputs = instance_model(image_tensor.unsqueeze(0).to(device))\n",
        "        pred_dict = inst_outputs[0]\n",
        "        masks = pred_dict['masks'].cpu()\n",
        "        labels = pred_dict['labels'].cpu()\n",
        "        scores = pred_dict['scores'].cpu()\n",
        "        keep_indices = scores > instance_score_thresh\n",
        "        filtered_instances = []\n",
        "        if keep_indices.any():\n",
        "            masks = masks[keep_indices]\n",
        "            labels = labels[keep_indices]\n",
        "            scores = scores[keep_indices]\n",
        "\n",
        "            for i in range(len(scores)):\n",
        "                filtered_instances.append({\n",
        "                    'masks': masks[i],\n",
        "                    'labels': labels[i],\n",
        "                    'scores': scores[i]\n",
        "                })\n",
        "        panoptic_result = panoptic_fusion(semantic_pred, filtered_instances)\n",
        "\n",
        "    return semantic_pred, filtered_instances, panoptic_result\n",
        "\n",
        "def get_cityscapes_color_map():\n",
        "    colors = np.array([\n",
        "        [128, 64, 128], [244, 35, 232], [70, 70, 70], [102, 102, 156], [190, 153, 153],\n",
        "        [153, 153, 153], [250, 170, 30], [220, 220, 0], [107, 142, 35], [152, 251, 152],\n",
        "        [70, 130, 180], [220, 20, 60], [255, 0, 0], [0, 0, 142], [0, 0, 70],\n",
        "        [0, 60, 100], [0, 80, 100], [0, 0, 230], [119, 11, 32], [0,0,0]\n",
        "    ], dtype=np.uint8)\n",
        "    full_colors = np.zeros((256, 3), dtype=np.uint8)\n",
        "    full_colors[:len(colors)] = colors\n",
        "    return full_colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gilTTCiCj3QT"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "CITYSCAPES_ROOT = \"./cityscapes\"\n",
        "NUM_EPOCHS_SEMANTIC = 15\n",
        "NUM_EPOCHS_INSTANCE = 15\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE_SEM = 1e-4\n",
        "LEARNING_RATE_INST = 1e-4\n",
        "MAX_SAMPLES_TRAIN = 300 \n",
        "MAX_SAMPLES_VAL = 100\n",
        "IMG_SIZE = (256, 512)\n",
        "SAVE_EPOCH = 5\n",
        "CHECKPOINT_DIR = \"./checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "semantic_losses = []\n",
        "instance_losses = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dataset_train = CityscapesPanopticDataset(CITYSCAPES_ROOT, 'train', get_transform(train=False, new_size=IMG_SIZE), max_samples=MAX_SAMPLES_TRAIN)\n",
        "dataset_val = CityscapesPanopticDataset(CITYSCAPES_ROOT, 'val', get_transform(train=False, new_size=IMG_SIZE), max_samples=MAX_SAMPLES_VAL)\n",
        "def train_collate_fn(batch):\n",
        "    images_transformed = torch.stack([item[0] for item in batch], 0)\n",
        "    semantic_targets_transformed = torch.stack([item[1] for item in batch], 0)\n",
        "    instance_targets_transformed = [item[2] for item in batch]\n",
        "    return images_transformed, semantic_targets_transformed, instance_targets_transformed\n",
        "\n",
        "dataloader_train_sem = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=train_collate_fn, drop_last=(BATCH_SIZE > 1 and len(dataset_train) % BATCH_SIZE !=0) )\n",
        "dataloader_train_inst = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=train_collate_fn, drop_last=(BATCH_SIZE > 1 and len(dataset_train) % BATCH_SIZE !=0) )\n",
        "dataloader_val_pq = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "\n",
        "semantic_model = get_semantic_model(num_classes=NUM_CITYSCAPES_CLASSES).to(DEVICE)\n",
        "instance_model = get_instance_model(num_thing_classes=NUM_CITYSCAPES_THING_CLASSES).to(DEVICE)\n",
        "optimizer_sem = optim.AdamW(semantic_model.parameters(), lr=LEARNING_RATE_SEM)\n",
        "optimizer_inst = optim.AdamW(instance_model.parameters(), lr=LEARNING_RATE_INST)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS_SEMANTIC):\n",
        "    avg_loss = train_semantic_one_epoch(semantic_model, optimizer_sem, dataloader_train_sem, DEVICE, epoch)\n",
        "    semantic_losses.append(avg_loss)\n",
        "    if (epoch + 1) % SAVE_EPOCH == 0 or (epoch + 1) == NUM_EPOCHS_SEMANTIC:\n",
        "        sem_checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"semantic_model_epoch_{epoch+1}.pth\")\n",
        "        torch.save({'epoch': epoch + 1, 'model_state_dict': semantic_model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer_sem.state_dict()}, sem_checkpoint_path)\n",
        "        print(f\"Saved semantic model {sem_checkpoint_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS_INSTANCE):\n",
        "    avg_loss = train_instance_one_epoch(instance_model, optimizer_inst, dataloader_train_inst, DEVICE, epoch)\n",
        "    instance_losses.append(avg_loss)\n",
        "    if (epoch + 1) % SAVE_EPOCH == 0 or (epoch + 1) == NUM_EPOCHS_INSTANCE:\n",
        "        inst_checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"instance_model_epoch_{epoch+1}.pth\")\n",
        "        torch.save({'epoch': epoch + 1, 'model_state_dict': instance_model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer_inst.state_dict()}, inst_checkpoint_path)\n",
        "        print(f\"Saved instance model {inst_checkpoint_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xRTN9tNdDVIY"
      },
      "outputs": [],
      "source": [
        "\n",
        "if semantic_losses or instance_losses:\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    if semantic_losses:\n",
        "        plt.subplot(1, 2, 1 if instance_losses else 1)\n",
        "        plt.plot(range(1, len(semantic_losses) + 1), semantic_losses, marker='o', label='Semantic Training Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Average Loss')\n",
        "        plt.title('Semantic Model Training Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "    if instance_losses:\n",
        "        plt.subplot(1, 2, 2 if semantic_losses else 1)\n",
        "        plt.plot(range(1, len(instance_losses) + 1), instance_losses, marker='x', color='r', label='Instance Training Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Average Loss')\n",
        "        plt.title('Instance Model Training Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(CHECKPOINT_DIR, \"training_losses.png\"))\n",
        "    print(f\"Saved training loss plot to {os.path.join(CHECKPOINT_DIR, 'training_losses.png')}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
